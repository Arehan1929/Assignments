{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Q1: What is Ensemble Learning in machine learning? Explain the key idea\n",
        "behind it.\n",
        "\n",
        "Answer:\n",
        "Ensemble learning combines predictions from multiple models (weak learners) to build a stronger overall predictor. By aggregating diverse models, ensembles reduce variance (bagging), reduce bias (boosting), or learn optimal combinations (stacking), typically improving generalization vs any single model."
      ],
      "metadata": {
        "id": "qyVqFhfCyiEV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q2: What is the difference between Bagging and Boosting?\n",
        "\n",
        "Answer:\n",
        "Bagging (Bootstrap Aggregating): Train base learners independently on bootstrapped samples; aggregate by averaging/voting. Reduces variance (good for high-variance, unstable learners like trees). Example: Random Forest.\n",
        "\n",
        "Boosting: Train learners sequentially; each new learner focuses on errors of the previous ones via reweighting/residuals. Reduces bias (can fit complex decision boundaries). Examples: AdaBoost, Gradient Boosting, XGBoost, LightGBM, CatBoost."
      ],
      "metadata": {
        "id": "VBQUxXwSyy4H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q3: What is bootstrap sampling and what role does it play in Bagging methods\n",
        "like Random Forest?\n",
        "\n",
        "Answer:\n",
        "Bootstrap sampling draws training sets with replacement from the original data (each bootstrap is the same size as original, so ~63.2% unique points). In bagging/Random Forest, each base tree is trained on a different bootstrap; predictions are averaged/voted, which decorrelates trees and reduces variance."
      ],
      "metadata": {
        "id": "yRoFaH6dzC7y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q4: What are Out-of-Bag (OOB) samples and how is OOB score used to\n",
        "evaluate ensemble models?\n",
        "\n",
        "Answer:\n",
        "For each bootstrap, points not selected (~36.8%) are Out-of-Bag for that model. We can predict each observation using only the models for which it was OOB and compute an OOB score (accuracy/R²). This gives a built-in, nearly unbiased validation without a separate holdout set."
      ],
      "metadata": {
        "id": "BcY57OVXzh6G"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q5: Compare feature importance analysis in a single Decision Tree vs a\n",
        "Random Forest.\n",
        "\n",
        "Answer:\n",
        "\n",
        "Single Tree: Importance comes from impurity decrease at splits. It can be unstable (high variance to small data perturbations).\n",
        "\n",
        "Random Forest: Importance is averaged over many trees → more stable/robust and less sensitive to random fluctuations."
      ],
      "metadata": {
        "id": "t-9Ho8vi0XcR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Q6: Write a Python program to:\n",
        "# ● Load the Breast Cancer dataset using\n",
        "# sklearn.datasets.load_breast_cancer()\n",
        "# ● Train a Random Forest Classifier\n",
        "# ● Print the top 5 most important features based on feature importance scores.\n",
        "\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "import numpy as np\n",
        "\n",
        "data = load_breast_cancer(as_frame=True)\n",
        "X, y = data.data, data.target\n",
        "\n",
        "rf = RandomForestClassifier(n_estimators=300, random_state=42, n_jobs=-1)\n",
        "rf.fit(X, y)\n",
        "\n",
        "imps = rf.feature_importances_\n",
        "names = np.array(X.columns)\n",
        "top = np.argsort(imps)[::-1][:5]\n",
        "for i, idx in enumerate(top, 1):\n",
        "    print(f\"{i}. {names[idx]}: {imps[idx]:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xT9T5UKi1xRp",
        "outputId": "257fe9d1-90c4-40c7-dd4b-a8ae090c6be8"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1. worst perimeter: 0.1476\n",
            "2. worst area: 0.1373\n",
            "3. worst concave points: 0.1209\n",
            "4. mean concave points: 0.0962\n",
            "5. worst radius: 0.0716\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Q7: Write a Python program to:\n",
        "# ● Train a Bagging Classifier using Decision Trees on the Iris dataset\n",
        "# ● Evaluate its accuracy and compare with a single Decision Tree\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "iris = load_iris(as_frame=True)\n",
        "X, y = iris.data, iris.target\n",
        "Xtr, Xte, ytr, yte = train_test_split(X, y, test_size=0.25, stratify=y, random_state=42)\n",
        "\n",
        "dt = DecisionTreeClassifier(random_state=42)\n",
        "dt.fit(Xtr, ytr)\n",
        "\n",
        "bag = BaggingClassifier(\n",
        "    estimator=DecisionTreeClassifier(random_state=42),\n",
        "    n_estimators=100,\n",
        "    random_state=42,\n",
        "    n_jobs=-1\n",
        ").fit(Xtr, ytr)\n",
        "\n",
        "print(\"Single DT acc:\", accuracy_score(yte, dt.predict(Xte)))\n",
        "print(\"Bagging acc  :\", accuracy_score(yte, bag.predict(Xte)))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gW4ALfAw2DWe",
        "outputId": "1f9b2478-79c1-4d36-f1a7-7144af11606f"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Single DT acc: 0.8947368421052632\n",
            "Bagging acc  : 0.9210526315789473\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Q8:  Write a Python program to:\n",
        "# ● Train a Random Forest Classifier\n",
        "# ● Tune hyperparameters max_depth and n_estimators using GridSearchCV\n",
        "# ● Print the best parameters and final accuracy\n",
        "\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "data = load_breast_cancer(as_frame=True)\n",
        "X, y = data.data, data.target\n",
        "Xtr, Xte, ytr, yte = train_test_split(X, y, test_size=0.25, stratify=y, random_state=42)\n",
        "\n",
        "rf = RandomForestClassifier(random_state=42, n_jobs=-1)\n",
        "param_grid = {\n",
        "    \"n_estimators\": [100, 200, 300],\n",
        "    \"max_depth\": [None, 5, 10, 15]\n",
        "}\n",
        "gs = GridSearchCV(rf, param_grid, cv=5, n_jobs=-1, scoring=\"accuracy\")\n",
        "gs.fit(Xtr, ytr)\n",
        "\n",
        "print(\"Best params:\", gs.best_params_)\n",
        "print(\"CV best acc:\", gs.best_score_)\n",
        "print(\"Test acc   :\", accuracy_score(yte, gs.best_estimator_.predict(Xte)))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KLL1Tfii2Yvd",
        "outputId": "8a17177d-7de6-41c9-8a39-591cfa64338a"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best params: {'max_depth': None, 'n_estimators': 300}\n",
            "CV best acc: 0.9624076607387142\n",
            "Test acc   : 0.958041958041958\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Write a Python program to:\n",
        "# ● Train a Bagging Regressor and a Random Forest Regressor on the California\n",
        "# Housing dataset\n",
        "# ● Compare their Mean Squared Errors (MSE)\n",
        "# If internet/data fetch is restricted, use a synthetic fallback.\n",
        "from sklearn.datasets import make_regression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.ensemble import BaggingRegressor, RandomForestRegressor\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "X, y = make_regression(n_samples=5000, n_features=8, noise=10.0, random_state=42)\n",
        "Xtr, Xte, ytr, yte = train_test_split(X, y, test_size=0.25, random_state=42)\n",
        "\n",
        "bag = BaggingRegressor(\n",
        "    estimator=DecisionTreeRegressor(random_state=42),\n",
        "    n_estimators=100,\n",
        "    random_state=42,\n",
        "    n_jobs=-1\n",
        ").fit(Xtr, ytr)\n",
        "\n",
        "rf = RandomForestRegressor(n_estimators=200, random_state=42, n_jobs=-1).fit(Xtr, ytr)\n",
        "\n",
        "print(\"Bagging MSE :\", mean_squared_error(yte, bag.predict(Xte)))\n",
        "print(\"RF MSE     :\", mean_squared_error(yte, rf.predict(Xte)))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K_NsMtgI3HF5",
        "outputId": "e6d2832c-518b-4429-b906-1b94b53c11e5"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bagging MSE : 1728.5747514160964\n",
            "RF MSE     : 1729.6498167436273\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q10: You are working as a data scientist at a financial institution to predict loan\n",
        "default. You have access to customer demographic and transaction history data.\n",
        "You decide to use ensemble techniques to increase model performance.\n",
        "Explain your step-by-step approach to:\n",
        "● Choose between Bagging or Boosting\n",
        "● Handle overfitting\n",
        "● Select base models\n",
        "● Evaluate performance using cross-validation\n",
        "● Justify how ensemble learning improves decision-making in this real-world\n",
        "context.\n",
        "\n",
        "Answer:\n",
        "\n",
        "When building a loan default prediction system for a financial institution, the goal is to identify customers who are most likely to default so that the bank can manage risk more effectively. The data includes customer demographics (like age, income, education) and transaction behavior (spending, repayment history, credit utilization). Here’s how I would approach it using ensemble learning:\n",
        "\n",
        "1. Choosing Between Bagging and Boosting\n",
        "\n",
        "I would first try a Random Forest (bagging) model because it’s simple, reliable, and gives a good baseline. Random Forest reduces variance and is less prone to overfitting compared to a single decision tree.\n",
        "After that, I’d move to Boosting methods (like XGBoost, LightGBM, or CatBoost), because they usually perform better on structured tabular data. Boosting focuses on the difficult-to-classify cases and often provides higher accuracy and recall, which is very important in catching defaulters.\n",
        "\n",
        "2. Handling Overfitting\n",
        "\n",
        "For Random Forest, I would control tree depth (max_depth), the minimum number of samples per leaf, and use the Out-of-Bag (OOB) score as an internal validation check.\n",
        "\n",
        "For Boosting, I’d use a validation set with early stopping, keep the learning rate small, and tune regularization parameters to avoid overly complex trees.\n",
        "\n",
        "3. Selecting Base Models\n",
        "\n",
        "The natural choice of base model is a Decision Tree since it handles both categorical and numerical features well.\n",
        "In addition, for more robustness, I might also try a stacking approach where I combine predictions from:\n",
        "\n",
        "Logistic Regression (for linear patterns),\n",
        "\n",
        "Random Forest (bagging), and\n",
        "\n",
        "XGBoost/LightGBM (boosting),\n",
        "then use a meta-model to blend them.\n",
        "\n",
        "4. Evaluating Performance\n",
        "\n",
        "Since loan defaults are usually rare (imbalanced dataset), accuracy alone won’t be enough. I would use:\n",
        "\n",
        "ROC-AUC to measure overall ranking ability,\n",
        "\n",
        "Precision-Recall curve to understand how well the model identifies defaulters,\n",
        "\n",
        "Recall at a fixed precision level, because missing too many defaulters is very costly for a bank.\n",
        "I’d also use Stratified K-Fold Cross Validation to make sure evaluation is consistent across different subsets of the data.\n",
        "\n",
        "5. Why Ensemble Learning Improves Decisions\n",
        "\n",
        "It reduces the risk of relying on one weak model. Bagging decreases variance, while boosting reduces bias.\n",
        "\n",
        "These models usually provide better generalization, meaning they perform more reliably on new, unseen customers.\n",
        "\n",
        "For the business, this translates into catching more potential defaulters early, lowering financial losses, and at the same time approving more safe loans — leading to both reduced risk and improved profits.\n",
        "\n",
        "Finally, tree-based ensembles can be explained with feature importance or SHAP values, which is important in the financial industry for transparency and regulatory compliance."
      ],
      "metadata": {
        "id": "Aj1tCNka3Ql6"
      }
    }
  ]
}